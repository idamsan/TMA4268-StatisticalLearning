---
title: "Compulsory exercise 2: 28"
author: "Lars A. M. Olsen, Ida M. Sandum, Ellen Skrimstad"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=5, fig.height=3,fig.align = "center")
```

```{r,eval=FALSE,echo=FALSE}
install.packages("ggplot2")
install.packages("tidyverse")
install.packages("palmerpenguins")
install.packages("GGally")
install.packages("MASS")
install.packages("caret")
install.packages("leaps")
install.packages("glmnet")
install.packages("pls")
install.packages("gam")
install.packages("e1071")
install.packages("tree")
install.packages("randomForest")
install.packages("ggfortify")
```
## Problem 1
# a)
Performing forward and backward stepwise selection:
```{r 1a, echo=TRUE, eval = TRUE}
library(MASS)
library(leaps)
# Hiding to save space as this was given in the project description
#str(Boston)

set.seed(1)
boston <- scale(Boston, center =T, scale =T)
train.ind = sample(1:nrow(boston), 0.8*nrow(boston))
boston.train = data.frame(boston[train.ind, ])
boston.test = data.frame(boston[-train.ind, ])
number_variables = 14

# Forward method
forward_method = regsubsets(medv~., data=boston.train,
                                nvmax =number_variables, method = "forward")
forward_method_summary = summary(forward_method)

#backward method
backward_method2 = regsubsets(medv~., data=boston.train, nvmax =number_variables, method = "backward")
backward_method2_summary = summary(backward_method2)

plot(forward_method_summary$adjr2, xlab="Number of variables", 
     ylab = "Adjusted RSq for forward", type="l")
plot(backward_method2_summary$adjr2, xlab="Number of variables", 
     ylab = "Adjusted RSq for backward", type="l")
```
# b)
We can find the four predictors forward stepwise selection considers the best by looking at the summary of the method. The four best are in decreasing order lstat, rn, ptratio and dis. 
```{r 1b, echo=TRUE, eval = TRUE}
forward_method_summary
```

# c) 
Now to run cross-validation with Lasso.
```{r c1, echo=TRUE, eval = TRUE}
library(glmnet)
set.seed(1)
x_train   <- model.matrix(medv~.,data= boston.train)
y_train   <- boston.train$medv

set.seed(1)
cv.out <- cv.glmnet(x_train, y_train, alpha=1, nfolds=5)
plot(cv.out)
```

ii) The best lambda is as printed below around 0.003. 
```{r c2, echo=TRUE, eval = TRUE}
best_lambda_lasso <- cv.out$lambda.min
best_lambda_lasso
```
iii) 
```{r c3, echo=TRUE, eval = TRUE}
coef(cv.out)
```

# d)
TRUE, FALSE, FALSE, TRUE

## Problem 2
To understand the data set we start by plotting a pairs-plot. 

```{r 2, echo=FALSE, eval = TRUE, results='hide'}
# load a synthetic data set
set.seed(1)
id <- "1CWZYfrLOrFdrIZ6Hv73e3xxt0SFgU4Ph" # google file ID
synthetic <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
# split into training and test sets
train.ind = sample(1:nrow(synthetic), 0.8 * nrow(synthetic)) 
synthetic.train = data.frame(synthetic[train.ind, ])
synthetic.test = data.frame(synthetic[-train.ind, ])
# show head(..)
# Y: response variable; X: predictor variable
#head(synthetic)
pairs(synthetic.train)
```
# a) 
```{r 2a, echo=TRUE, eval = TRUE}
pcr_model <- pcr(Y~., data = synthetic.train, scale = TRUE, validation="CV")
validationplot(pcr_model, val.type ="MSEP")
```

```{r 2a2, echo=TRUE, eval = TRUE}
library(pls)
plsr_model <- plsr(Y~., data=synthetic.train,scale=TRUE, validation="CV")
validationplot(plsr_model,val.type="MSEP")
```
# b) 
From the plots we can see that for PLSR the mean square error of prediction drops low for far fewer components.For  PLSR the error end up at 0.4 after only adding 4 components, while for PCR this happens only after adding all ten components.

PCR is unsupervised, that is it does not use the values for Y to choose directions. Because of the it in does not necessarily choose the direction which explains the correlation to the response first, but instead the direction which explains the most variance in the covariates. This is also the main difference between PCR and PLS, that PLS includes the response Y when choosing directions. Thus we can assume that in general PLS model decreases MSEP for lower numbers of components in the case that some of the covariates are strongly related to the response.  

Specifically for this data set we can see from the pairs plot above that there is correlation between Y and  $X_1$ and between $X_2$ and $X_3$. This explains the difference between the two plots, namely how PLS has a large drop after adding the first component, because it chooses this one as $X_1$, while PCR most likely chooses a direction to explain most of both $X_2$ and $X_3$ first and as such does not have a similar drop. In fact PCR does not explain any of the correlation to the response in the first component. 

## Problem 3
# a)
TRUE, FALSE, FALSE, TRUE
# b)
```{r ,eval=TRUE,echo=FALSE}
boston <- scale(Boston, center = T, scale = T)
train.ind = sample(1:nrow(boston), 0.8 * nrow(boston))
boston.train = data.frame(boston[train.ind, ])
boston.test = data.frame(boston[-train.ind, ])
```

```{r ,eval=TRUE,echo=TRUE}
library(gam)
additive_model <- gam(medv ~ rm + s(ptratio,df=3) + poly(lstat,degree = 2), data=boston.train)
par(mfrow=c(1,3))
plot(additive_model, se=T)
```
## Problem 4
# a) 
FALSE, TRUE, TRUE, TRUE

# b)  
See figure 1.
![4b) Tree corresponding to the partition of the predictor space illustrated in the assigment description](\Users\Bruker\Documents\MTFYMA\tredje_år\StatLær\R\Øvinger\regression tree.PNG)

# c)
```{r, problem 4c, eval=TRUE,echo=TRUE, fig.width=5, fig.height=4}
library(tidyverse)
library(tree)
library(palmerpenguins)  # Contains the data set 'penguins'.
data(penguins)

names(penguins) <- c("species", "island", "billL", "billD", "flipperL", "mass", "sex", 
                     "year")

Penguins_reduced <- penguins %>% dplyr::mutate(mass = as.numeric(mass), flipperL = as.numeric(flipperL), 
                                               year = as.numeric(year)) %>% drop_na()

# We do not want 'year' in the data (this will not help for future predictions)
Penguins_reduced <- Penguins_reduced[, -c(8)]

set.seed(4268)
# 70% of the sample size for training set
training_set_size <- floor(0.7 * nrow(Penguins_reduced))
train_ind <- sample(seq_len(nrow(Penguins_reduced)), size = training_set_size)
train <- Penguins_reduced[train_ind, ]
test <- Penguins_reduced[-train_ind, ]

# 4c, i) 

# Simple classification tree using Gini index
data.tree = tree(formula=species ~ ., data = train, split="gini")
plot(data.tree, type="uniform")
text(data.tree,pretty=1, cex= 0.5)
title("Classification tree")
```
```{r, eval=TRUE, echo=TRUE}
# ii) 
# Cost-complexity pruning 10-fold CV
set.seed(123)
data.cvtree = cv.tree(data.tree, FUN=prune.misclass, K=10)
plot(data.cvtree$size, data.cvtree$dev, type = "b")
``` 
We see from the plot that the deviation is lowest for 3 terminal nodes. 
```{r, eval=TRUE,echo=TRUE}
# iii)
# New tree using 3 terminal nodes
prune.data <- prune.misclass(data.tree, best = 3)
plot(prune.data, type="uniform")
text(prune.data, pretty = 1, cex = 0.5)
``` 
From the two plots of the trees we see that flipper length and bill length and bill depth are used in the top splits.
```{r, eval=TRUE,echo=TRUE}
# Predicting species 
pred.prune = predict(prune.data, newdata = test, type = "class")
response.test = Penguins_reduced$species[-train_ind]

# Misclassification table
misclass.prune = table(pred.prune, response.test)
misclass.prune
error_rate = 1 - sum(diag(misclass.prune))/sum(misclass.prune)
```
We see that the misclassification error rate is `r error_rate`. 

# d) 
Now we are going to use a more advanced method, and we have chosen to use random forests. We let the number of trees be 500, as a large number of trees will not lead to overfitting in the case of random forests. We also choose $m=\sqrt{p}$ as the number of predictors to be considered at each split. We want this number to be rather low, in case there is a very strong predictor in the data set. We do not want such a predictor to overshadow the rest of them. Considering only a subset of the predictors is a way to overcome this problem. 
```{r, eval=TRUE, echo=TRUE}
# d)
library(randomForest)

# Using the random forest method 
rf.penguins = randomForest(species ~ ., data = train, mtry = round(sqrt(ncol(Penguins_reduced)-1)) -
    1, ntree = 500, importance = TRUE)

varImpPlot(rf.penguins)
```
In this plot we see that bill length, flipper length and bill depth are important variables, which was expected from the previous trees. We also see that island has a high mean decrease accuracy. This was not expected from the trees, as island has not been a split. Combining previous information and this new information, we therefore say bill length and flipper length are the two most important covariates. 

```{r, eval=TRUE, echo=TRUE}
# Predicting species using random forest
pred.rf = predict(rf.penguins, newdata = test)

misclass.rf = table(pred.rf, response.test)
misclass.rf
error_rf = 1 - sum(diag(misclass.rf))/sum(misclass.rf)

``` 

The misclassification error rate for the test data is now `r error_rf`, which is an improvement from before.  

## Problem 5


# a)
FALSE, TRUE, TRUE, TRUE

# b)

```{r, eval=TRUE, echo=TRUE}
# 5b)
library(e1071)

set.seed(123)

# 10 fold cross-validation to find cost parameter for linear boundary
CV_linear = tune(svm, species ~ ., data = train, kernel = "linear", ranges = list(cost = c(0.001, 
    0.01, 0.1, 1, 5, 10, 50)))
best_model_linear = CV_linear$best.model
cost_linear = CV_linear$best.parameters
error_linear = CV_linear$best.performance

# 10 fold cross-validation to find cost and gamma parameter for radial boundary
CV_kernel = tune(svm, species ~ ., data = train, kernel = "radial", ranges = list(cost = c(0.01, 
    0.1, 1, 5, 10, 100, 1000), gamma = c(0.01, 0.1, 1, 10, 100)))
best_model_radial = CV_kernel$best.model
cost_kernel = CV_kernel$best.parameters$cost
gamma_kernel = CV_kernel$best.parameters$gamma
error_radial = CV_kernel$best.performance
```
We see that the best cost when classifying using linear boundary is `r cost_linear`, and this gives an error rate of `r error_linear`. Using radial boundary we have a cost of `r cost_kernel`, a gamma of `r gamma_kernel`, and an error rate of `r error_radial`. 


```{r, eval=TRUE, echo=TRUE}
# Prediction of species using linear boundary
pred_linear = predict(best_model_linear, test)
misclass_linear = table(pred_linear, response.test)
error_linear_pred = 1 - sum(diag(misclass_linear))/sum(misclass_linear)

# Prediction of species using radial boundary
pred_radial = predict(best_model_radial, test)
misclass_radial = table(pred_radial, response.test)
error_radial_pred = 1 - sum(diag(misclass_radial))/sum(misclass_radial)

misclass_linear
misclass_radial
```
From the misclassification tables we see that one of the penguins is misclassified using linear boundary, and none are misclassified using radial boundary. The misclassification error rates are `r error_linear_pred` and `r error_radial_pred`, respectively. 

Using this information it seems like a radial classifier boundary is better, and we therefore prefer that one in the case of the penguins data set. This obviously depends on the data set, and is not a general decision. 


## Problem 6 

```{r ,eval=TRUE,echo=FALSE}
set.seed(1)
# load a synthetic dataset
id <- "1NJ1SuUBebl5P8rMSIwm_n3S8a7K43yP4" # google file ID
happiness <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id),fileEncoding="UTF-8-BOM")

```

```{r ,eval=TRUE,echo=FALSE}
cols = c('Country.name', 
         'Ladder.score',  # happiness score
         'Logged.GDP.per.capita',  
         'Social.support', 
         'Healthy.life.expectancy', 
         'Freedom.to.make.life.choices',
         'Generosity',  # how generous people are
         'Perceptions.of.corruption')

# We continue with a subset of 8 columns:
happiness = subset(happiness, select = cols)
rownames(happiness) <- happiness[, c(1)]

# And we creat an X and a Y matrix
happiness.X = happiness[, -c(1, 2)]
happiness.Y = happiness[, c(1, 2)]
happiness.XY = happiness[, -c(1)]

# scale
happiness.X = data.frame(scale(happiness.X))

```

```{r ,eval=TRUE,echo=TRUE}
library(ggfortify)
pca_mat = prcomp(happiness.X, center = T, scale = T)

# Score and loadings plot:
autoplot(pca_mat, data = happiness.X, colour = "Black", loadings = TRUE, loadings.colour = "red", 
    loadings.label = TRUE, loadings.label.size = 5, label = T, label.size = 4.5)
```
# a)
i)
Two characteristics observed in relations between the variables are:
- We see that PC2 has a significant effect on "Generosity", wile PC1 hardly has any effect. "Logged GDP per capita", "Social support" and "Healthy life expectancy", on the other hand, has a low effect from PC2, but high effect from PC1. This indicates that there is a low correlation between the variables generosity and the three others.
- We also observe that "Freedom to make life choices" and "perception of corruption" are on opposite sides of the plot. This indicates that these are different. So, if a country has one, it does not have much of the other, since they almost are completely different in both PC1 and PC2.


ii)
Afghanistan can be consider to an outlier among the countries in the selection.

# b)
i)
```{r ,eval=TRUE,echo=TRUE, fig.width=5, fig.height=5}
first_princ <- data.frame(pca_mat$rotation)$PC1

?barplot()

barplot(abs(first_princ), names.arg = rownames(pca_mat$rotation),legend = colnames(happiness.XY)[-1], col = c("red","green","blue","yellow","purple","black","grey"), args.legend = list(x="topright",cex=.7))
```
ii)
```{r ,eval=TRUE,echo=TRUE}
library(pls)
plsr_model <- plsr(Ladder.score ~ ., data=happiness.XY, scale=T)
```

iii)
```{r ,eval=TRUE,echo=TRUE, fig.width=5, fig.height=5}
plsr_plot <- plsr_model$loadings[,c('Comp 1')]
barplot(abs(plsr_plot),legend = colnames(happiness.XY)[-1], col = c("red","green","blue","yellow","purple","black","grey"), args.legend = list(x="topright",cex=.7))
```

```{r,eval=FALSE,echo=FALSE}
abs(plsr_plot)
abs(first_princ)
```

We see that these plots are relatively similar. But if we look at the numerical values for each, we see that the biggest difference is that Generosity is slightly smaller for the PLSR, than it is for PCA. For the other variables the numerical values are almost the same for PLSR and PCA.

iv)
The three most important predictors to predict the happiness score based on the PLSR bar graph from iii) are 
  - logged GDP per capita
  - Social support
  - Healthy life expectancy
These are the bars that are largest.

# c)
FALSE, FALSE, FALSE, TRUE 

# d)
i)
```{r ,eval=TRUE,echo=TRUE}
library(stats)
set.seed(1)
K = 4  # your choice
km.out = kmeans(happiness.X, K)

autoplot(pca_mat, data = happiness.X, colour = km.out$cluster, label = T, label.size = 5, 
    loadings = F, loadings.colour = "blue", loadings.label = F, loadings.label.size = 3)
```


ii)
```{r ,eval=TRUE,echo=FALSE}

set.seed(1)
cluster1 <- km.out$cluster == 1
countries_1 = which(cluster1==TRUE)
cluster2 <- km.out$cluster == 2
countries_2 = which(cluster2==TRUE)
cluster3 <- km.out$cluster == 3
countries_3 = which(cluster3==TRUE)
cluster4 <- km.out$cluster == 4
countries_4 = which(cluster4==TRUE)
```

Printing the countries in each cluster, we find that:
- cluster 1: countries in black
- cluster 2: countries in red
- cluster 3: countries in green
- cluster 4: countries in blue

Then we print the average happiness score for each cluster:

```{r ,eval=TRUE,echo=TRUE}
set.seed(1)
average_lst <- list(1:K)
average_lst
for (i in 1:K){
  cluster_nr <- km.out$cluster == i
  country = which(cluster_nr==TRUE)
  average = mean(happiness.XY[country, ]$Ladder.score)
  average_lst[i] = average
}
average_lst
```
From this plot we see that the blue cluster, 4, containing countries such as Norway, Iceland, Denmark and New Zealand, has the highest average on the happiness score. While the black cluster, 1, containing Afghanistan and South-America, has the lowest average.

If we look at the plot, we see a pattern with happiness and distribution of countries along the x-axis. The happiest cluster is the blue cluster, where the countries in general are far left in the plot. The second happiest cluster, the green cluster, has a distribution of countries in the middle left of the plot. The two clusters left, black and red, has approximately the same happiness score, which is low, and these countries are in centered around the middle right of the plot. 

We conclude that for a given country, happiness increases the further to the left the country is located in the plot.




